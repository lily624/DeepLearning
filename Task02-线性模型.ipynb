{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b232cf",
   "metadata": {},
   "source": [
    "# 线性模型\n",
    "\n",
    "## 基本形式\n",
    "\n",
    "线性模型试图学得一个通过属性的线性组合来进行预测的函数，即$f(x) = w^{T}x + b$, 其中$w = (w_{1},w_{2},...,w_{d})$. $w$直观表达各个属性在预测中的重要性，可以理解为属性的权重，因此线性模型有很好的可解释性（comprehensibility）。\n",
    "\n",
    "在西瓜的例子里，$f_{good}(x) = 0.2 * x_{色泽}+0.5 * x_{根蒂} + 0.3 * x_{敲声} +1$, 也就是通过考虑色泽、根蒂和敲声来判断瓜好不好，其中根蒂是最重要的（0.5最大），敲声比色泽更重要。\n",
    "\n",
    "\n",
    "## 经典线性模型\n",
    "\n",
    "假设数据集$D = {(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})}$,其中$x_{i} =(x_{i1},x_{i2},...,x_{id}), y_{i}\\in R$\n",
    "\n",
    "**1. 回归任务**\n",
    "\n",
    "线性回归试图学习得到一个线性模型以尽可能准确的预测实值输出标记。前面说到，线性回归试图学习$f(x) = w^{T}x + b$，如果确定$w$和$b$，关键在于如何衡量$f(x)$和$y$之间的差别。\n",
    "\n",
    "线性回归中，**最小二乘法（least square method）** 就是试图找到一条直线，是的所有的样本到直线上的欧式距离之和最小，或者说最小化均方误差（mean sqaure error）,即\n",
    "\n",
    "$ (w^{*},b^{*}) = argmin_{(w,b)} \\sum_{i=1}^{m} (f(x_{i})-y_{i})^2 = argmin_{(w,b)} \\sum_{i=1}^{m} (y_{i} - wx_{i} - b)^2$\n",
    "\n",
    "求解$w$和$b$使得上面的式子最小化的过程，就叫线性回归模型的最小二乘“参数估计”parameter estimation.\n",
    "\n",
    "\n",
    "**对数线性回归（log-linear regression)**\n",
    "\n",
    "将输出标记的对数作为线性模型逼近的目标，即$\\ln y = w^{T}x + b$,实际上是试图让$e^{w^{T}x+b}$逼近$y$.\n",
    "\n",
    "**广义线性模型(generalized linear model)**\n",
    "\n",
    "考虑一个单调可微的函数$g(·)$， 令\n",
    "\n",
    "$ y = g^{-1}(w^{T}x + b)$\n",
    "\n",
    "这里$g(·)$称为link function.\n",
    "\n",
    "**2. 二分类任务**\n",
    "\n",
    "**对数几率回归**\n",
    "只需在前面的广义线性模型中找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来，就可以做分类任务。二分类任务，顾名思义，输出标记$y$只有0和1。线性回归中的$y/z$的预测值是一个实值，所以我们只需要把这个实值转换为0/1值。\n",
    "\n",
    "**单位阶跃函数** \n",
    "$$ y = \\begin{cases}\n",
    "0 & z<0 \\\\\n",
    "0.5 & z=0 \\\\\n",
    "1 & z>0 \n",
    "\\end{cases}$$\n",
    "\n",
    "也就是如果预测值大于0，则判定为正例；预测值小于0，则判定为反例；预测值为0，可随意判定。\n",
    "\n",
    "单位阶跃函数是不连续的，我们希望找到一个函数近似单位阶跃函数并且是单调可微的，于是就引出了几率函数（lostic function）:\n",
    "\n",
    "$$ y = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "对数函数是一种“Sigmoid函数”。这一类方法很多优点，比如直接对分类可能性进行建模，不需要事先假设数据分布，可以避免假设数据分布带来的问题。\n",
    "\n",
    "\n",
    "**极大似然估计Maximum likelihood method**\n",
    "\n",
    "有一种很常见的策略是先假定其具有某种确定的概率分布形式，然后再基于训练样本对概率分布的参数进行估计。是一种“模型已定，参数未知”的方法，假设P(x|c)具有某种确定的形式并且被参数$\\theta_{c}$唯一确定，那我们就是利用训练集D估计参数$\\theta_{c}$。\n",
    "最大似然法估计参数的过程一般是：\n",
    "\n",
    "    写出似然函数\n",
    "    \n",
    "    对似然函数取对数，并整理\n",
    "    \n",
    "    求导数，令偏导数为0，得到似然方程组\n",
    "    \n",
    "    解似然方程组，得到所有参数即为所求\n",
    "\n",
    "\n",
    "\n",
    "**线性判别分析（Linear Discriminant Analysis, LDA）** \n",
    "\n",
    "LDA的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上,使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adaccdb",
   "metadata": {},
   "source": [
    "\n",
    "**3. 多分类任务**\n",
    "\n",
    "多分类学习的基本思路是“拆解法”，即多分类任务拆为若干个二分类任务求解。\n",
    "\n",
    "经典的拆分策略有：\n",
    "\n",
    "x1. One vs. One 简化OvO.\n",
    "x2. One vs. Rest 简称OvR\n",
    "x3. Many vs. Many 简称MvM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ba8fb",
   "metadata": {},
   "source": [
    "**4. 类别不平衡问题**\n",
    "\n",
    "不同类别的训练样例数差别大会对学习过程造成困扰。类别不平衡就是指分类任务中不同类别的训练样例数目差别很大的情况。在使用OvR、MvM策略后产生的二分类任务仍可能出现类别不平衡现象，因此有必要了解类别不平衡性处理的基本方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5223ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
