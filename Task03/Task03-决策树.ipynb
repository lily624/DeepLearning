{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3184264",
   "metadata": {},
   "source": [
    "# 基本流程\n",
    "\n",
    "决策树是一类常见的机器学习方法。决策树是基于树结构来进行决策的。\n",
    "\n",
    ".例如，我们要对“这是好瓜吗?”这样的问题进行决策时，通常会进行一系列的判断或“子决策”：我们先看“它是什么颜色?”，如果是“青绿色”，则我们再看“它的根蒂是什么形态?”，如果是“蜷缩”，我们再判断“它敲起来是什么声音?”，最后，我们得出最终决策：这是个好瓜.\n",
    "\n",
    "一般的，一颗决策树包含一个根节点、若干个内部节点和若干个叶节点；叶节点对应于决策结果，其他每个结点则对应于一个属性的测试，每个节点包含的样本集合根据属性测试结果划分到子结点中；根节点包含整个样本集。这种流程叫“分而治之”（divide-and-conquer)\n",
    "\n",
    "决策树是一个递归的过程。有三种情况导致递归返回:\n",
    "\n",
    "1. 当前节点包含的样本所有属性属于同一类别，无需划分；\n",
    "\n",
    "2. 当前属性集为空，所有样本在所有属性上的取值相同，无法划分；\n",
    "\n",
    "    解决：把当前节点标记为叶结点，将其类别设定为该节点所含样本最多的类别；（当前节点的后验分布）\n",
    "\n",
    "3. 当前节点包含的样本集合为空，不能划分。\n",
    "    \n",
    "    解决：把当前节点标记为叶节点，将其类别设定为父结点所含样本最多的类别。（当前节点的先验分布）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b67151",
   "metadata": {},
   "source": [
    "# 划分选择\n",
    "\n",
    "## 信息增益\n",
    "   \n",
    "### 熵 entropy\n",
    "\n",
    "    熵是随机变量不确定的度量。\n",
    "\n",
    "    假设X是一个有限状态的离散型随机变量，其概率分布为： \n",
    "$$P(X=x_{i}) = p_{i}, i=1,2,...,n$$\n",
    "\n",
    "    那么随机变量X的熵定义为： \n",
    "    \n",
    "$$H(X) = - \\sum_{i=1}^{n} p_{i} log(p_{i})$$\n",
    "\n",
    "    熵越大，则随机变量的不确定性就越大** 或者说 **熵越大，随机变量的纯度（purity）就越低\n",
    "\n",
    "\n",
    "    当随机变量只有0，1两个取值时:\n",
    "\n",
    "    假设$P(X=1) = p$, 则随机变量X的熵定义为 \n",
    "$$H(X) = -plog(p) -(1-p)log(1-p)$$\n",
    "    \n",
    "    求导可知，当p=0.5时，熵取值最大，随机变量不确定性最大。\n",
    "\n",
    "\n",
    "#### 条件熵\n",
    "\n",
    "    随机变量X给定的条件下，随机变量Y的条件熵$H(Y|X)$定义为：\n",
    "\n",
    "$$H(Y|X) = \\sum_{i=1}^{n} p_{i} H(Y|X=x_{i})$$\n",
    "    \n",
    "    其中，$p_{i} = P(X = x_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f7949",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 信息增益\n",
    "\n",
    "根据信息增益准则进行特征选择的方法是：对训练数据集D，计算其每个特征的信息增益，并比它们的大小，从而选择信息增益最大的特征.\n",
    "\n",
    "以信息增益作为特征选择准则，会存在偏向于选择取值较多的特征的问题。可以采用信息增益比对这一问题进行校正。\n",
    "\n",
    "根据信息增益准则进行特征选择的方法是：对训练数据集D，计算其每个特征的信息增益，并比它们的大小，从而选择信息增益最大的特征。\n",
    "\n",
    "**假设训练数据集为D，样本容量为|D|,有$k$个类别$C_{k}$,$|C_{k}|$为类别$C_{k}$的样本个数。某一特征A有n个不同的取值$a_{1},...,a_{n}$。根据特征A的取值可将数据集D划分为n个子集$D_{1}，...,D_{n}$,$|D_{i}|$为$D_{i}$的样本个数。并记子集$D_{i}$中属于类$C_{k}$的样本的集合为$D_{ik}$,$|D_{ik}|$为$D_{ik}$的样本个数。**\n",
    "\n",
    "\n",
    "假设样本第k类样本所占的比例为$p_{k}(k=1,2,...|Y|)$,那么信息熵则为：\n",
    "\n",
    "$$H(D) = - \\sum_{k=1}^{|Y|} p_{k} \\log_{2}(p_{k})$$\n",
    "\n",
    "### 信息熵增\n",
    "\n",
    "\n",
    "得知特征X的信息而使得类Y的信息的不确定性减少的程度。\n",
    "\n",
    "特征A对训练数据集D的信息增益$g(D,A)$定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差，即\n",
    "\n",
    "$$g(D, A) = H(D) - H(D|A)$$\n",
    "\n",
    "### 增益率\n",
    "\n",
    "简单使用信息增益作为准则有一个较大的问题，就是它更偏好取值较多的属性。例如，当相貌这个属性有帅和不帅两个取值，而身家这个属性有贫穷，小康，中产，富裕四个取值，那么信息增益会更偏好身家这个属性。为了减少这个英雄，C4.5使用增益率来选择最优划分属性。\n",
    "\n",
    "特征A对训练数据集D的**信息增益率**定义为其信息增益与训练集D关于特征A的值的熵之比，即\n",
    "\n",
    "$$gR(D|A) = \\frac{g(D,A)}{H_{A}(D)}$$\n",
    "\n",
    "其中，$$H_{A}(D) = -\\sum_{i=1}^{n} \\frac{|Di|}{|D|}log(\\frac{|Di|}{|D|})$$\n",
    "\n",
    "**C4.5决策树算法**[Quinlan, 1993]就不直接使用信息增益，而是使用“增益率”(gain ratio/gR)来选择最优划分属性."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ac208cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对y的各种可能的取值出现的个数进行计数.。其他函数利用该函数来计算数据集和的混杂程度\n",
    "def uniquecounts(rows):\n",
    "    results = {}\n",
    "    for row in rows:\n",
    "        #计数结果在最后一列\n",
    "        r = row[len(row)-1]\n",
    "        if r not in results:results[r] = 0\n",
    "        results[r]+=1\n",
    "    return results # 返回一个字典\n",
    "\n",
    "# 熵\n",
    "\n",
    "def entropy(rows):\n",
    "    from math import log\n",
    "    log2 = lambda x:log(x)/log(2)\n",
    "    results = uniquecounts(rows)\n",
    "    #开始计算熵的值\n",
    "    ent = 0.0\n",
    "    for r in results.keys():\n",
    "        p = float(results[r])/len(rows)\n",
    "        ent = ent - p*log2(p)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda8e70",
   "metadata": {},
   "source": [
    "#### 1.6 ID3 算法和C4.5算法\n",
    "\n",
    "ID3算法的核心是在决策树的各个结点上应用信息增益准则进行特征选择。具体做法是：\n",
    "\n",
    "\n",
    "从根节点开始，对结点计算所有可能特征的信息增益，选择信息增益最大的特征作为结点的特征，并由该特征的不同取值构建子节点；\n",
    "对子节点递归地调用以上方法，构建决策树；\n",
    "直到所有特征的信息增益均很小或者没有特征可选时为止。\n",
    "\n",
    "C4.5算法与ID3算法的区别主要在于它在生产决策树的过程中，使用信息增益比来进行特征选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b81562b",
   "metadata": {},
   "source": [
    "### 基尼指数（GINI INDEX）\n",
    "\n",
    "**CART决策树**就是使用基尼指数来选择划分属性。\n",
    "\n",
    "\n",
    "数据集D的基尼值：$$Gini(D) = \\sum_{i=1}^{n}\\sum_{i'\\neq i} p_{i}p_{i'} = 1 - \\sum_{i=1}^{n} p_{k}^{2}$$\n",
    "\n",
    "Gini(D)反映的是从数据集D中随机抽取两个样本，其类别标志不一致的概率。所以Gini（D）越小，D的纯度就越高。\n",
    "\n",
    "属性a的基尼指数则定义为：\n",
    "\n",
    "$ Gini Index(D,a) = \\sum_{v=1}^{V}\\frac{|D^{v}|}{|D|} Gini(D^{v})$\n",
    "\n",
    "或者说，假设有k个类别，样本点属于第k类的概率为$p_{k}$,则概率分布的基尼指数定义为：\n",
    "\n",
    "$Gini(p) = \\sum_{i=1}^{K} p_{i}(1-p_{i}) = 1 - \\sum_{k=1}^{K} p_{k}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5217038",
   "metadata": {},
   "source": [
    "### 2.1 CART决策树\n",
    "\n",
    "CART假设决策树是一个二叉树，它通过递归地二分每个特征，将特征空间划分为有限个单元，并在这些单元上确定预测的概率分布。CART算法中，对于回归树，采用的是平方误差最小化准则；对于分类树，采用基尼指数最小化准则。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc95e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decisionnode:\n",
    "    def __init__(self, col=-1, value = None, results = None, tb = None, fb = None):\n",
    "        self.col = col #col是待检验的判断条件所对应的列索引值\n",
    "        self.value = value # value 对应于为了使结果为true， 当前列必须匹配的值\n",
    "        self.results = results #保存的是针对当前分支的结果，它是一个字典\n",
    "        self.tb = tb # decision node，对应于结果为true，树上相对于当前节点的子树上的节点\n",
    "        self.fb = fb # desision node,对应于结果为true时，树上相对于当前节点的子树上的节点\n",
    "        \n",
    "        \n",
    "def giniimpurity(rows):\n",
    "    total = len(rows)\n",
    "    counts = uniquecounts(rows)\n",
    "    imp =0\n",
    "    for k1 in counts:\n",
    "        p1 = float(counts[k1])/total\n",
    "        for k2 in counts: # 这个循环是否可以用（1-p1）替换？\n",
    "            if k1 == k2: continue\n",
    "            p2 = float(counts[k2])/total\n",
    "            imp+=p1*p2\n",
    "    return imp\n",
    "\n",
    "\n",
    "def giniimpurity_2(rows):\n",
    "    total = len(rows)\n",
    "    counts = uniquecounts(rows)\n",
    "    imp = 0\n",
    "    for k1 in counts.keys():\n",
    "        p1 = float(counts[k1])/total\n",
    "        imp+= p1*(1-p1)\n",
    "    return imp\n",
    "\n",
    "# 在某一列上对数据集进行拆分，可应用于数值型或因子型变量\n",
    "def divideset(rows,column, value):\n",
    "    #定义一个函数，判断当前数据行属于第一组还是第二组\n",
    "    split_function = None\n",
    "    if isinstance(value,int) or isinstance(value,float):\n",
    "        split_function = lambda row:row[column] >= value\n",
    "    else:\n",
    "        split_function = lambda row:row[column]==value\n",
    "    # 将数据集拆分成两个集合，并返回\n",
    "    set1 = [row for row in rows if split_function(row)]\n",
    "    set2 = [row for row in rows if not split_function(row)]\n",
    "    return(set1,set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c7d227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以递归方式构造树\n",
    "\n",
    "def buildtree(rows, scoref = entropy):\n",
    "    if len(rows) ==0: return decisionode()\n",
    "    current_score = scoref(rows)\n",
    "    \n",
    "    #定义一个变量来记录最佳拆分条件\n",
    "    best_gain = 0.0\n",
    "    best_criteria = None\n",
    "    best_sets = None\n",
    "    \n",
    "    column_count = len(rows[0]) - 1\n",
    "    for col in range(0, column_count):\n",
    "        # 在当前列中生成一个由不同值构成的序列\n",
    "        column_values = {}\n",
    "        for row in rows:\n",
    "            column_values[row[col]] = 1 # initialzation\n",
    "        \n",
    "        # try to split the dataset according to the value in this column \n",
    "        for value in column_values.keys():\n",
    "            (set1,set2) = divideset(rows, col, value)\n",
    "            \n",
    "            # information gain\n",
    "            p = float(len(set1))/len(rows)\n",
    "            gain = current_score - p * scoref(set1) - (1-p)*scoref(set2)\n",
    "            if gain > best_gain and len(set1) >0 and len(set2) >0:\n",
    "                best_gain = gain\n",
    "                best_criteria = (col, value)\n",
    "                best_sets = (set1, set2)\n",
    "                \n",
    "    # create sub branch\n",
    "    if best_gain>0:\n",
    "        trueBranch = buildtree(best_sets[0])\n",
    "        falseBranch = buildtree(best_sets[1])\n",
    "        return decisionnode(col = best_criteria[0], value = best_criteria[1], tb = trueBranch, fb = falseBranch)\n",
    "\n",
    "    else:\n",
    "        return decisionnode(results = uniquecounts(rows))\n",
    "    \n",
    "\n",
    "# dicision tree visualization\n",
    "\n",
    "def printtree(tree, indent = ''):\n",
    "    # if tree node or not\n",
    "    if tree.results != None:\n",
    "        print(str(tree.results))\n",
    "    else:\n",
    "        # print conditions\n",
    "        print(str(tree.col) +\":\"+str(tree.value)+\"?\")\n",
    "        # print indent\n",
    "        print(indent+\"T->\")\n",
    "        printtree(tree.tb, indent+\"\")\n",
    "        print(indent+\"F->\")\n",
    "        printtree(tree.fb,indent+\" \")       \n",
    "        \n",
    "\n",
    "# classify new observations\n",
    "\n",
    "def classify(observation,tree):\n",
    "    if tree.results!= None:\n",
    "        return tree.results\n",
    "    else:\n",
    "        v = observation[tree.col]\n",
    "        branch = None\n",
    "        if isinstance(v,int) or isinstance(v,float):\n",
    "            if v>= tree.value: branch = tree.tb\n",
    "            else: branch = tree.fb\n",
    "        else:\n",
    "            if v==tree.value : branch = tree.tb\n",
    "            else: branch = tree.fb\n",
    "        return classify(observation,branch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "777a13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data=[['slashdot','USA','yes',18,'None'],\n",
    "        ['google','France','yes',23,'Premium'],\n",
    "        ['digg','USA','yes',24,'Basic'],\n",
    "        ['kiwitobes','France','yes',23,'Basic'],\n",
    "        ['google','UK','no',21,'Premium'],\n",
    "        ['(direct)','New Zealand','no',12,'None'],\n",
    "        ['(direct)','UK','no',21,'Basic'],\n",
    "        ['google','USA','no',24,'Premium'],\n",
    "        ['slashdot','France','yes',19,'None'],\n",
    "        ['digg','USA','no',18,'None'],\n",
    "        ['google','UK','no',18,'None'],\n",
    "        ['kiwitobes','UK','no',19,'None'],\n",
    "        ['digg','New Zealand','yes',12,'Basic'],\n",
    "        ['slashdot','UK','no',21,'None'],\n",
    "        ['google','UK','yes',18,'Basic'],\n",
    "        ['kiwitobes','France','yes',19,'Basic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10b66453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['slashdot', 'USA', 'yes', 18, 'None'],\n",
       "  ['google', 'France', 'yes', 23, 'Premium'],\n",
       "  ['digg', 'USA', 'yes', 24, 'Basic'],\n",
       "  ['kiwitobes', 'France', 'yes', 23, 'Basic'],\n",
       "  ['slashdot', 'France', 'yes', 19, 'None'],\n",
       "  ['digg', 'New Zealand', 'yes', 12, 'Basic'],\n",
       "  ['google', 'UK', 'yes', 18, 'Basic'],\n",
       "  ['kiwitobes', 'France', 'yes', 19, 'Basic']],\n",
       " [['google', 'UK', 'no', 21, 'Premium'],\n",
       "  ['(direct)', 'New Zealand', 'no', 12, 'None'],\n",
       "  ['(direct)', 'UK', 'no', 21, 'Basic'],\n",
       "  ['google', 'USA', 'no', 24, 'Premium'],\n",
       "  ['digg', 'USA', 'no', 18, 'None'],\n",
       "  ['google', 'UK', 'no', 18, 'None'],\n",
       "  ['kiwitobes', 'UK', 'no', 19, 'None'],\n",
       "  ['slashdot', 'UK', 'no', 21, 'None']])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divideset(my_data,2,'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9c678ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6328125"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giniimpurity(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "156a601d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6328125"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giniimpurity_2(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3989c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:google?\n",
      "T->\n",
      "3:21?\n",
      "T->\n",
      "{'Premium': 3}\n",
      "F->\n",
      "2:no?\n",
      " T->\n",
      "{'None': 1}\n",
      " F->\n",
      "{'Basic': 1}\n",
      "F->\n",
      "0:slashdot?\n",
      " T->\n",
      "{'None': 3}\n",
      " F->\n",
      "2:yes?\n",
      "  T->\n",
      "{'Basic': 4}\n",
      "  F->\n",
      "3:21?\n",
      "   T->\n",
      "{'Basic': 1}\n",
      "   F->\n",
      "{'None': 3}\n"
     ]
    }
   ],
   "source": [
    "tree = buildtree(my_data)\n",
    "printtree(tree = tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "650a8f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Basic': 4}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(['(direct)','USA','yes',5],tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69faa2d6",
   "metadata": {},
   "source": [
    "# 剪枝处理\n",
    "\n",
    "如果对训练集建立完整的决策树，会使得模型过于针对训练数据，拟合了大部分的噪声，即出现过度拟合的现象。为了避免这个问题，有两种解决的办法：预剪枝和后剪枝。 剪枝(pruning)是决策树学习算法对付“过拟合”的主要手段。\n",
    "\n",
    "剪枝方法和程度对决策树泛化性能的影响相当显著。\n",
    "\n",
    "## 预剪枝“prepruning”\n",
    "\n",
    "对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化能力的提升，则停止划分，当前结点标记为叶结点。\n",
    "\n",
    "如何评估决策树泛化能力？性能评估方法，比如留出法，预留一部分数据用作“验证集”以进行性能评估。预剪枝会让很多分支都没有“展开”，降低了过拟合的风险，还减少了决策树的训练时间开销和测试时间开销。但也有“欠拟合”的风险。\n",
    "\n",
    "\n",
    "## 后剪枝“post-pruning”\n",
    "\n",
    "先从训练集生成一颗完整的决策树，然后自底向上对非叶结点进行考察，如果该结点对应的字树替换为叶结点能带来决策树泛化能力提升，把该子树替换为叶结点。\n",
    "\n",
    "后剪枝通常比预剪枝决策树保留了更多的分支，一般，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但是后剪枝过程是生成完全决策树之后进行的，训练时间开销要大很多。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763da6e",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "决策树的剪枝通过极小化决策树整体的损失函数来实现。在提高信息增益的基础上，对模型的复杂度T施加惩罚，就是损失函数的定义：\n",
    "\n",
    "$$C_{\\alpha}(T) = \\sum_{t=1}^{|T|} N_{t} H_{t}(T) + \\alpha |T|$$\n",
    "\n",
    "$\\alpha$的大小反映了对模型训练集拟合度和模型复杂度的折衷考虑。剪枝的过程就是当$\\alpha$确定时，选择损失函数最小的模型。\n",
    "\n",
    "步骤如下：\n",
    "\n",
    "1. 计算每个节点的经验熵；\n",
    "\n",
    "2. 递归地从树的叶节点向上回缩，如果将某一个父节点的所有叶节点合并，能够使得其损失函数减小，则进行剪枝，将父节点变成新的叶节点；\n",
    "\n",
    "3. 返回2，直到不能继续合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb4f36ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:google?\n",
      "T->\n",
      "3:21?\n",
      "T->\n",
      "{'Premium': 3}\n",
      "F->\n",
      "2:no?\n",
      " T->\n",
      "{'None': 1}\n",
      " F->\n",
      "{'Basic': 1}\n",
      "F->\n",
      "0:slashdot?\n",
      " T->\n",
      "{'None': 3}\n",
      " F->\n",
      "2:yes?\n",
      "  T->\n",
      "{'Basic': 4}\n",
      "  F->\n",
      "3:21?\n",
      "   T->\n",
      "{'Basic': 1}\n",
      "   F->\n",
      "{'None': 3}\n"
     ]
    }
   ],
   "source": [
    "# 决策树的剪枝\n",
    "\n",
    "def prune(tree,mingain):\n",
    "    # 如果分支不是叶节点，则对其进行剪枝\n",
    "    if tree.tb.results == None:\n",
    "        prune(tree.tb,mingain)\n",
    "    if tree.fb.results == None:\n",
    "        prune(tree.fb,mingain)\n",
    "    # 如果两个子分支都是叶节点，判断是否能够合并\n",
    "    if tree.tb.results !=None and tree.fb.results !=None:\n",
    "        #构造合并后的数据集\n",
    "        tb,fb = [],[]\n",
    "        for v,c in tree.tb.results.items():\n",
    "            tb+=[[v]]*c\n",
    "        for v,c in tree.fb.results.items():\n",
    "            fb+=[[v]]*c\n",
    "        #检查熵的减少量\n",
    "        delta = entropy(tb+fb)-(entropy(tb)+entropy(fb)/2)\n",
    "        if delta < mingain:\n",
    "            # 合并分支\n",
    "            tree.tb,tree.fb = None,None\n",
    "            tree.results = uniquecounts(tb+fb)\n",
    "# test\n",
    "tree = buildtree(my_data,scoref = giniimpurity)\n",
    "prune(tree,0.1)\n",
    "printtree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e712f",
   "metadata": {},
   "source": [
    "# 连续值和缺失值\n",
    "\n",
    "## 连续值\n",
    "\n",
    "到目前为止我们讨论的都是离散属性，但是现实任务中我们不可避免的会遇到连续属性，此时我们应该如何处理呢？\n",
    "\n",
    "C4.5算法给出的方案是用二分法来对连续属性进行处理。假设样本集为D，其中有一连续属性a,假设a在D上有n个不同的取值，我们将其由小到大排序$a_{1},a_{2},...,a_{n}$。 我们设定一个划分点t将D分为两个子集$D_{t}^{−}$,$D_{t}^{+}$，前者包含属性a上小于等于t的样本，后者包含大于t的样本。显然这里t在两个相邻a值之间取任意值不会影响划分结果，那么t值就有n−1个候选项。通常我们可将划分点设为各个区间的中位点，即：\n",
    "\n",
    "$$ T_{a} = \\{ \\frac{a^{i}+a^{i+1}}{2}|1\\leq i \\leq n-1 \\}$$\n",
    "\n",
    "那么我们就可以像离散值一样考察这些划分点，例如当使用信息增益时：\n",
    "\n",
    "$$Gain(D,a) = max Gain(D,a,t) = max \\ H(D) - \\sum_{\\lambda \\in \\{ -, + \\}} \\frac{|D_{t}^{\\lambda}|}{|D|} H(D_{t}^{\\lambda})$$\n",
    "\n",
    "**连续属性被用来在某个节点做划分之后，仍然可以在后续节点被用到，这和离散属性不同。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0b0b4",
   "metadata": {},
   "source": [
    "## 缺失值\n",
    "现实任务中经常遇到属性值确实，比如因为测量成本、隐私保护等因素，患者的医疗数据可能在某些属性上是未知的。如果简单的放弃不完整样本，是对数据信息极大的浪费，训练出来的模型性能也会受到影响。\n",
    "\n",
    "- 问题1： 如何在属性值缺失的情况下进行划分属性选择？\n",
    "\n",
    "- 问题2：给定划分属性，若样本在该属性上的缺失值，如何对样本进行划分？\n",
    "\n",
    "\n",
    "\n",
    "### 划分属性的选择\n",
    "\n",
    "在不具有缺失值的数据集中，对于某一个属性a，算法一般使用其所有样本来判断属性的优劣。自然地，对于具有缺失值的数据集，在某一个属性a下，算法只能使用在属性a下不具有缺失值的样本子集（即下面的$\\tilde{D}$）来判断属性a的优劣。那么，缺失样本所占的比例是否影响算法对属性的选择？\n",
    "\n",
    "<mark style = background-color:yellow>对于数据集$D$,如果没有缺失值，则以信息增益最大的属性作为最优划分属性；假如样本有缺失值，其信息增益就是：无缺失值样本的比例$\\rho$乘以无缺失值样本子集$\\tilde{D}$的信息增益。</mark>\n",
    "\n",
    "这说明，对于某一个属性，如果缺失值比例越大，信息增益越不重要。\n",
    "\n",
    "\n",
    "假设  表示在数据集$D$的属性a上，不具有缺失值的样本构成的样本子集；$\\tilde{D^{v}}$表示在$\\tilde{D}$的属性a上取值为$a^{v}$时构成的样本子集；  表示在$\\tilde{D_{k}}$中属于第k类的样本子集。由此，可以确定以下变量（初始时$w_{x}=1$):\n",
    "\n",
    "$\\rho = \\frac{\\sum_{ x\\in\\tilde{D}} w_{x}}{\\sum_{x\\in D}w_{x}}$\n",
    "\n",
    "$\\tilde{\\rho_{k}} = \\frac{\\sum_{ x\\in\\tilde{D_{k}}} w_{x}}{\\sum_{x\\in \\tilde{D}}w_{x}}$\n",
    "\n",
    "$\\tilde{r_{v}} = \\frac{\\sum_{ x\\in\\tilde{D^{v}}} w_{x}}{\\sum_{x\\in D}w_{x}}$\n",
    "\n",
    "\n",
    "那么，具有缺失值的样本集的信息增益公式就变成了：\n",
    "\n",
    "$$ Gain(D, a) = \\rho * Gain(\\tilde{D}, a) = \\rho * (H(D) - \\sum_{v=1}^{V} \\tilde{r_{v}} H(\\tilde{D^{v}}))$$\n",
    "\n",
    "\n",
    "### 样本划分\n",
    "\n",
    "因为我们不知道缺失值原本是什么，所以我们将该样本划分到所有属性分支下；但是，划分到不同分支后，该样本的权重$w_{x}$也不再相同——这个样本被划分到属性值为$a^{v}$的分支后的权重将被更新为\n",
    "\n",
    "$$ w_{x}:= \\tilde{r_{v}} w_{x}$$\n",
    "\n",
    "即根据“先验”来决定划分到不同分支的重要程度，这个样本在下一次划分属性的选择时，将以更新后的权重值参与$\\rho, \\tilde{\\rho_{k}},\\tilde{r_{v}}$ 的再计算。同样地，对于在属性a上无缺失值的样本，被划分到子结点时的权重保持不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6629e",
   "metadata": {},
   "source": [
    "# 多变量决策树\n",
    "\n",
    "\n",
    "如果把每个属性当成坐标空间中的一个坐标轴，那d个属性描述的样本就对应了d维空间中的一个数据点，对样本分类则意味着这个坐标空间中寻找到不同类样本之间的分类边界。决策树所形成的的分类边界有一个明显的特点： 轴平行(axis-parallel)，也就是它的分类边界由若干个与坐标轴平行的分段组成。\n",
    "\n",
    "“多变量决策树”的划分不再是与坐标轴平行的线段，而是实现“斜划分”，甚至更复杂划分的决策树。以斜划分为例，此类决策树中，非叶结点不再试仅对某个属性，而是对属性的线性组合进行测试,是试图建立一个合适的线性分类器。\n",
    "\n",
    "多变量决策树算法主要有**OC1**和Brodley and Utgoff提出的一系列算法。**OC1**先贪心地寻找每个属性的最优权值，在局部优化的基础上在对分类边界进行随机扰动以试图找到更好的边界。还有一些则引入了线性分类器学习的最小二乘法，还有一些试图在决策树的叶结点上嵌入神经网络，以结合两种学习机制的有事，如果“感知机树”在决策树的每个叶结点上训练一个感知机。\n",
    "\n",
    "算法**ID4、ITI** 通过调整分支路径上的划分属性次序来对树进行部分重构，即在对接收到新样本后可对已学习的模型进行调整，而不用完全重新学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8834b",
   "metadata": {},
   "source": [
    "# 决策树的优缺点\n",
    "\n",
    "## 优点\n",
    "\n",
    "- 易于理解和解释，甚至比线性回归更直观；\n",
    "- 与人类做决策思考的思维习惯契合；\n",
    "- 模型可以通过树的形式进行可视化展示；\n",
    "- 可以直接处理非数值型数据，不需要进行哑变量的转化，甚至可以直接处理含缺失值的数据；\n",
    "\n",
    "\n",
    "## 缺点\n",
    "\n",
    "- 对于有大量数值型输入和输出的问题，决策树未必是一个好的选择；\n",
    "- 特别是当数值型变量之间存在许多错综复杂的关系，如金融数据分析；\n",
    "- 决定分类的因素取决于更多变量的复杂组合时；\n",
    "- 模型不够稳健，某一个节点的小小变化可能导致整个树会有很大的不同。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
